"""
æ•°æ®é‡‡é›†ä¸å…¥åº“æ¨¡å—
ä» urls.json è¯»å– URLï¼ŒæŠ“å–ç½‘é¡µï¼Œç”Ÿæˆ LangChain Documentï¼Œåˆ‡åˆ†ï¼Œembeddingï¼Œå†™å…¥ FAISS
"""
import json
import os
from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

from utils.html_loader import load_webpage
from utils.text_cleaner import clean_text
from rag.retriever import get_embeddings
from config import CHUNK_SIZE, CHUNK_OVERLAP


def load_urls_from_json(json_path: str = "./data/urls.json") -> List[Dict[str, Any]]:
    """
    ä» JSON æ–‡ä»¶åŠ è½½ URL åˆ—è¡¨
    
    Args:
        json_path: JSON æ–‡ä»¶è·¯å¾„
    
    Returns:
        URL é…ç½®åˆ—è¡¨
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"URL é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
    
    with open(json_path, "r", encoding="utf-8") as f:
        urls = json.load(f)
    
    return urls


def check_url_domain_allowed(url: str) -> bool:
    """
    æ£€æŸ¥ URL æ˜¯å¦åœ¨ç™½åå•åŸŸåä¸­
    
    Args:
        url: è¦æ£€æŸ¥çš„ URL
    
    Returns:
        å¦‚æœ URL åœ¨ç™½åå•ä¸­è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    allowed_domains = [
        "gov.sg",
        "hdb.gov.sg",
        "cea.gov.sg",
        "ura.gov.sg",
    ]
    
    from urllib.parse import urlparse
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å…è®¸çš„åŸŸå
    for allowed in allowed_domains:
        if domain == allowed or domain.endswith("." + allowed):
            return True
    
    return False


def ingest_documents(
    urls: List[Dict[str, Any]] = None,
    chunk_size: int = None,
    chunk_overlap: int = None,
    persist_directory: str = "./data/faiss",
    index_name: str = "singapore_rental",
) -> None:
    """
    æ‰§è¡Œæ•°æ®é‡‡é›†ä¸å…¥åº“æµç¨‹
    
    Args:
        urls: URL é…ç½®åˆ—è¡¨ï¼ˆå¦‚æœä¸º Noneï¼Œä» urls.json è¯»å–ï¼‰
        chunk_size: æ–‡æ¡£åˆ‡åˆ†å¤§å°ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨ config.py ä¸­çš„é»˜è®¤å€¼ï¼‰
        chunk_overlap: æ–‡æ¡£åˆ‡åˆ†é‡å ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨ config.py ä¸­çš„é»˜è®¤å€¼ï¼‰
        persist_directory: FAISS æŒä¹…åŒ–ç›®å½•
        index_name: FAISS ç´¢å¼•åç§°
    """
    # ä½¿ç”¨ config.py ä¸­çš„é»˜è®¤å€¼
    if chunk_size is None:
        chunk_size = CHUNK_SIZE
    if chunk_overlap is None:
        chunk_overlap = CHUNK_OVERLAP
    # åŠ è½½ URL åˆ—è¡¨
    if urls is None:
        urls = load_urls_from_json()
    
    print(f"ğŸ“‹ å…± {len(urls)} ä¸ª URL å¾…å¤„ç†")
    
    # æ­¥éª¤ 1: æŠ“å–ç½‘é¡µå¹¶ç”Ÿæˆ Document
    all_documents = []
    failed_urls = []
    
    for i, url_config in enumerate(urls, 1):
        url = url_config.get("url", "")
        title = url_config.get("title", "")
        
        print(f"\n[{i}/{len(urls)}] å¤„ç†: {title}")
        print(f"   URL: {url}")
        
        # æ£€æŸ¥åŸŸåç™½åå•
        if not check_url_domain_allowed(url):
            print(f"   âš ï¸  è­¦å‘Š: URL ä¸åœ¨ç™½åå•ä¸­ï¼Œè·³è¿‡")
            failed_urls.append({"url": url, "reason": "åŸŸåä¸åœ¨ç™½åå•"})
            continue
        
        try:
            # åŠ è½½ç½‘é¡µ
            doc = load_webpage(url)
            
            # æ¸…ç†æ–‡æœ¬
            cleaned_content = clean_text(doc.page_content)
            print(f"   ğŸ“ æ¸…ç†åå†…å®¹é•¿åº¦: {len(cleaned_content)} å­—ç¬¦")
            if not cleaned_content or len(cleaned_content) < 100:
                print(f"   âš ï¸  è­¦å‘Š: æå–çš„å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡")
                failed_urls.append({"url": url, "reason": "å†…å®¹è¿‡çŸ­"})
                continue
            
            # æ›´æ–° metadata
            doc.metadata.update({
                "title": title or doc.metadata.get("title", ""),
                "category": url_config.get("category", ""),
            })
            doc.page_content = cleaned_content
            
            all_documents.append(doc)
            print(f"   âœ… æˆåŠŸ: æå– {len(cleaned_content)} å­—ç¬¦")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            failed_urls.append({"url": url, "reason": str(e)})
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª URLï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
            continue
    
    if not all_documents:
        print("\nâŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ–‡æ¡£")
        if failed_urls:
            print("\nå¤±è´¥çš„ URL:")
            for item in failed_urls:
                print(f"  - {item['url']}: {item['reason']}")
        return
    
    print(f"\nâœ… æˆåŠŸæå– {len(all_documents)} ä¸ªæ–‡æ¡£")
    
    # æ­¥éª¤ 2: æ–‡æœ¬åˆ‡åˆ†
    print(f"\nâœ‚ï¸  åˆ‡åˆ†æ–‡æ¡£ (chunk_size={chunk_size}, chunk_overlap={chunk_overlap})...")
    
    # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„é•¿åº¦
    for i, doc in enumerate(all_documents, 1):
        print(f"   æ–‡æ¡£ {i} ({doc.metadata.get('title', 'æœªçŸ¥')}): {len(doc.page_content)} å­—ç¬¦")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    split_docs = text_splitter.split_documents(all_documents)
    print(f"âœ… åˆ‡åˆ†ä¸º {len(split_docs)} ä¸ª chunks")
    
    # æ˜¾ç¤ºæ¯ä¸ª chunk çš„é•¿åº¦åˆ†å¸ƒ
    if split_docs:
        chunk_lengths = [len(doc.page_content) for doc in split_docs]
        print(f"   Chunk é•¿åº¦ç»Ÿè®¡: æœ€å°={min(chunk_lengths)}, æœ€å¤§={max(chunk_lengths)}, å¹³å‡={sum(chunk_lengths)/len(chunk_lengths):.0f}")
    
    # æ­¥éª¤ 3: ç”Ÿæˆ Embeddings å¹¶å†™å…¥ FAISS
    print(f"\nğŸ”¢ ç”Ÿæˆ Embeddings å¹¶å†™å…¥å‘é‡åº“...")
    print("   (é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…)")
    
    embeddings = get_embeddings()
    
    # å¦‚æœå‘é‡åº“å·²å­˜åœ¨ï¼ŒåŠ è½½å¹¶æ·»åŠ æ–°æ–‡æ¡£
    # FAISS ä¿å­˜çš„æ–‡ä»¶æ˜¯ .faiss å’Œ .pkl
    faiss_path = os.path.join(persist_directory, f"{index_name}.faiss")
    pkl_path = os.path.join(persist_directory, f"{index_name}.pkl")
    if os.path.exists(faiss_path) and os.path.exists(pkl_path):
        print(f"   ğŸ“‚ åŠ è½½ç°æœ‰å‘é‡åº“: {persist_directory}")
        try:
            vectorstore = FAISS.load_local(
                persist_directory,
                embeddings,
                allow_dangerous_deserialization=True,
            )
            # æ·»åŠ æ–°æ–‡æ¡£
            vectorstore.add_documents(split_docs)
            print(f"   âœ… å·²æ·»åŠ  {len(split_docs)} ä¸ªæ–° chunks åˆ°ç°æœ‰å‘é‡åº“")
        except Exception as e:
            print(f"   âš ï¸  åŠ è½½ç°æœ‰å‘é‡åº“å¤±è´¥ï¼Œåˆ›å»ºæ–°å‘é‡åº“: {e}")
            vectorstore = FAISS.from_documents(
                documents=split_docs,
                embedding=embeddings,
            )
            print(f"   âœ… åˆ›å»ºæ–°å‘é‡åº“ï¼ŒåŒ…å« {len(split_docs)} ä¸ª chunks")
    else:
        # åˆ›å»ºæ–°å‘é‡åº“
        vectorstore = FAISS.from_documents(
            documents=split_docs,
            embedding=embeddings,
        )
        print(f"   âœ… åˆ›å»ºæ–°å‘é‡åº“ï¼ŒåŒ…å« {len(split_docs)} ä¸ª chunks")
    
    # æŒä¹…åŒ–
    os.makedirs(persist_directory, exist_ok=True)
    vectorstore.save_local(persist_directory, index_name=index_name)
    print(f"   ğŸ’¾ å‘é‡åº“å·²ä¿å­˜åˆ°: {persist_directory}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š å…¥åº“å®Œæˆç»Ÿè®¡:")
    print(f"   - æˆåŠŸå¤„ç†: {len(all_documents)} ä¸ªæ–‡æ¡£")
    print(f"   - ç”Ÿæˆ chunks: {len(split_docs)} ä¸ª")
    print(f"   - å¤±è´¥ URL: {len(failed_urls)} ä¸ª")
    
    if failed_urls:
        print(f"\nâš ï¸  å¤±è´¥çš„ URL:")
        for item in failed_urls:
            print(f"   - {item['url']}: {item['reason']}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®é‡‡é›†ä¸å…¥åº“")
    parser.add_argument(
        "--urls", 
        type=str,
        default="./data/urls.json",
        help="URL é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=CHUNK_SIZE,
        help="æ–‡æ¡£åˆ‡åˆ†å¤§å°",
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=CHUNK_OVERLAP,
        help="æ–‡æ¡£åˆ‡åˆ†é‡å ",
    )
    parser.add_argument(
        "--persist-dir",
        type=str,
        default="./data/faiss",
        help="FAISS æŒä¹…åŒ–ç›®å½•",
    )
    
    args = parser.parse_args()
    
    # ä» JSON æ–‡ä»¶åŠ è½½ URLs
    urls = load_urls_from_json(args.urls)
    
    # æ‰§è¡Œå…¥åº“
    ingest_documents(
        urls=urls,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        persist_directory=args.persist_dir,
        index_name="singapore_rental",
    )


