"""
RAG ç³»ç»Ÿè¯„æµ‹æ¨¡å—
æ‰¹é‡è¿è¡Œè¯„æµ‹é—®é¢˜ï¼Œç”Ÿæˆè¯„æµ‹æŠ¥å‘Š
"""
import json
import os
from typing import List, Dict, Any
from datetime import datetime

from llm.deepseek_llm import get_deepseek_llm
from rag.retriever import load_vectorstore, create_retriever
from rag.chain import run_rag_query
from config import (
    FAISS_PERSIST_DIR,
    FAISS_INDEX_NAME,
    INITIAL_RETRIEVAL_K,
    EVALUATION_QUESTIONS_PATH,
)


def load_evaluation_questions(json_path: str = None) -> List[Dict[str, Any]]:
    """
    åŠ è½½è¯„æµ‹é—®é¢˜
    
    Args:
        json_path: è¯„æµ‹é—®é¢˜ JSON æ–‡ä»¶è·¯å¾„
    
    Returns:
        è¯„æµ‹é—®é¢˜åˆ—è¡¨
    """
    if json_path is None:
        json_path = EVALUATION_QUESTIONS_PATH
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"è¯„æµ‹é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
    
    with open(json_path, "r", encoding="utf-8") as f:
        questions = json.load(f)
    
    return questions


def evaluate_rag_system(
    questions: List[Dict[str, Any]] = None,
    output_report: str = "./evaluation_report.md",
) -> Dict[str, Any]:
    """
    è¯„æµ‹ RAG ç³»ç»Ÿ
    
    Args:
        questions: è¯„æµ‹é—®é¢˜åˆ—è¡¨ï¼ˆå¦‚æœä¸º Noneï¼Œä»æ–‡ä»¶åŠ è½½ï¼‰
        output_report: è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    
    Returns:
        è¯„æµ‹ç»“æœå­—å…¸
    """
    # åŠ è½½è¯„æµ‹é—®é¢˜
    if questions is None:
        questions = load_evaluation_questions()
    
    print(f"ğŸ“‹ åŠ è½½äº† {len(questions)} ä¸ªè¯„æµ‹é—®é¢˜")
    
    # æ£€æŸ¥å‘é‡åº“æ˜¯å¦å­˜åœ¨
    vectorstore = load_vectorstore(
        persist_directory=FAISS_PERSIST_DIR,
        index_name=FAISS_INDEX_NAME,
    )
    
    if vectorstore is None:
        raise ValueError(
            f"å‘é‡åº“ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ python ingest.py è¿›è¡Œæ•°æ®é‡‡é›†ã€‚"
        )
    
    print(f"âœ… å‘é‡åº“åŠ è½½æˆåŠŸ")
    
    # åˆ›å»º LLM å’Œ Retriever
    try:
        llm = get_deepseek_llm()
        print(f"âœ… LLM åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        raise ValueError(f"LLM åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # ä½¿ç”¨åˆå§‹æ£€ç´¢æ•°é‡ï¼ˆé‡æ’åºä¼šåœ¨ chain å†…éƒ¨å¤„ç†ï¼‰
    retriever = create_retriever(vectorstore, k=INITIAL_RETRIEVAL_K)
    print(f"âœ… Retriever åˆ›å»ºæˆåŠŸ (åˆå§‹æ£€ç´¢ k={INITIAL_RETRIEVAL_K}ï¼Œé‡æ’åºåè¿”å› 8 æ¡)")
    
    # æ‰§è¡Œè¯„æµ‹
    results = []
    success_count = 0
    no_citation_count = 0
    
    print(f"\nğŸš€ å¼€å§‹è¯„æµ‹...\n")
    
    for i, q_config in enumerate(questions, 1):
        question = q_config.get("question", "")
        category = q_config.get("category", "unknown")
        
        print(f"[{i}/{len(questions)}] {question}")
        
        try:
            # è¿è¡Œ RAG æŸ¥è¯¢
            result = run_rag_query(question, llm, retriever)
            
            answer = result.get("answer", "")
            citations = result.get("citations", [])
            
            # åˆ¤æ–­æ˜¯å¦æœ‰å¼•ç”¨
            has_citations = len(citations) > 0
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸï¼ˆæœ‰å¼•ç”¨ä¸”å›ç­”ä¸ä¸ºç©ºï¼‰
            is_success = has_citations and len(answer.strip()) > 0
            
            if is_success:
                success_count += 1
            if not has_citations:
                no_citation_count += 1
            
            results.append({
                "question": question,
                "category": category,
                "answer": answer,
                "citations": citations,
                "has_citations": has_citations,
                "is_success": is_success,
            })
            
            status = "âœ…" if is_success else "âš ï¸"
            citation_status = f"å¼•ç”¨: {len(citations)}" if has_citations else "æ— å¼•ç”¨"
            print(f"   {status} {citation_status}")
            
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")
            results.append({
                "question": question,
                "category": category,
                "answer": "",
                "citations": [],
                "has_citations": False,
                "is_success": False,
                "error": str(e),
            })
    
    # ç”ŸæˆæŠ¥å‘Š
    success_rate = (success_count / len(questions)) * 100 if questions else 0
    citation_rate = ((len(questions) - no_citation_count) / len(questions)) * 100 if questions else 0
    
    # æ‰¾å‡ºå¤±è´¥æ ·ä¾‹ï¼ˆè‡³å°‘ 5 æ¡ï¼Œæˆ–æ‰€æœ‰å¤±è´¥çš„ï¼‰
    failed_results = [r for r in results if not r.get("is_success", False)]
    failed_samples = failed_results[:max(5, len(failed_results))]
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    report_lines = [
        "# RAG ç³»ç»Ÿè¯„æµ‹æŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "## æ€»ä½“ç»Ÿè®¡",
        "",
        f"- **æ€»é—®é¢˜æ•°**: {len(questions)}",
        f"- **æˆåŠŸå›ç­”**: {success_count} ({success_rate:.1f}%)",
        f"- **æœ‰å¼•ç”¨**: {len(questions) - no_citation_count} ({citation_rate:.1f}%)",
        f"- **æ— å¼•ç”¨**: {no_citation_count}",
        "",
        "## å¤±è´¥æ ·ä¾‹",
        "",
    ]
    
    if failed_samples:
        for i, result in enumerate(failed_samples, 1):
            report_lines.extend([
                f"### å¤±è´¥æ ·ä¾‹ {i}",
                "",
                f"**é—®é¢˜**: {result['question']}",
                f"**åˆ†ç±»**: {result.get('category', 'unknown')}",
                f"**æ˜¯å¦æœ‰å¼•ç”¨**: {'æ˜¯' if result.get('has_citations') else 'å¦'}",
                "",
            ])
            
            if result.get("error"):
                report_lines.append(f"**é”™è¯¯**: {result['error']}")
            else:
                answer = result.get("answer", "")
                if answer:
                    report_lines.append(f"**å›ç­”**: {answer[:200]}...")
                else:
                    report_lines.append("**å›ç­”**: (ç©º)")
            
            report_lines.append("")
    else:
        report_lines.append("æ— å¤±è´¥æ ·ä¾‹ã€‚")
        report_lines.append("")
    
    # è¯¦ç»†ç»“æœ
    report_lines.extend([
        "## è¯¦ç»†ç»“æœ",
        "",
        "| # | é—®é¢˜ | åˆ†ç±» | æœ‰å¼•ç”¨ | æˆåŠŸ |",
        "|---|------|------|--------|------|",
    ])
    
    for i, result in enumerate(results, 1):
        question_short = result["question"][:50] + "..." if len(result["question"]) > 50 else result["question"]
        has_cit = "âœ…" if result.get("has_citations") else "âŒ"
        is_success = "âœ…" if result.get("is_success") else "âŒ"
        report_lines.append(
            f"| {i} | {question_short} | {result.get('category', 'unknown')} | {has_cit} | {is_success} |"
        )
    
    report_content = "\n".join(report_lines)
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_report, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    print(f"\nğŸ“Š è¯„æµ‹å®Œæˆ:")
    print(f"   - æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"   - å¼•ç”¨ç‡: {citation_rate:.1f}%")
    print(f"   - å¤±è´¥æ ·ä¾‹: {len(failed_samples)} ä¸ª")
    print(f"   - æŠ¥å‘Šå·²ä¿å­˜: {output_report}")
    
    return {
        "total": len(questions),
        "success_count": success_count,
        "success_rate": success_rate,
        "citation_count": len(questions) - no_citation_count,
        "citation_rate": citation_rate,
        "failed_samples": failed_samples,
        "results": results,
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG ç³»ç»Ÿè¯„æµ‹")
    parser.add_argument(
        "--questions",
        type=str,
        default=None,
        help="è¯„æµ‹é—®é¢˜ JSON æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./evaluation_report.md",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„",
    )
    
    args = parser.parse_args()
    
    # æ‰§è¡Œè¯„æµ‹
    evaluate_rag_system(
        questions=None if args.questions is None else load_evaluation_questions(args.questions),
        output_report=args.output,
    )

